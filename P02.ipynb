{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTUgL25hGQZq"
      },
      "outputs": [],
      "source": [
        "configs = [\n",
        "    # Word2Vec\n",
        "    {\n",
        "        'experiment_name': 'Unidirectional_Word2Vec_LR_0.01',\n",
        "        'no_layers': 1,\n",
        "        'hidden_dim': 128,\n",
        "        'embedding_dim': 100,\n",
        "        'freeze_embeddings': True,\n",
        "        'bidirectional': False,\n",
        "        'epochs': 50,\n",
        "        'learning_rate': 0.01,\n",
        "        'dropout': 0.5,\n",
        "        'clip': 5,\n",
        "        'embedding_type': 'word2vec',\n",
        "        'embedding_path': None\n",
        "    },\n",
        "    # GloVe 100d\n",
        "    {\n",
        "        'experiment_name': 'GloVe_100d',\n",
        "        'no_layers': 1,\n",
        "        'hidden_dim': 128,\n",
        "        'embedding_dim': 100,\n",
        "        'freeze_embeddings': True,\n",
        "        'bidirectional': False,\n",
        "        'epochs': 50,\n",
        "        'learning_rate': 0.001,\n",
        "        'dropout': 0.5,\n",
        "        'clip': 5,\n",
        "        'embedding_type': 'glove',\n",
        "        'embedding_path': None\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGW8Q8zRK2is",
        "outputId": "2f3336ff-d109-4781-80e0-c5ebdf7ce5f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-18 03:29:35--  http://faculty.cooper.edu/sable2/courses/fall2024/ece467/TC_provided.tar.gz\n",
            "Resolving faculty.cooper.edu (faculty.cooper.edu)... 199.98.16.192\n",
            "Connecting to faculty.cooper.edu (faculty.cooper.edu)|199.98.16.192|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3421989 (3.3M) [application/x-gzip]\n",
            "Saving to: ‘TC_provided.tar.gz’\n",
            "\n",
            "TC_provided.tar.gz  100%[===================>]   3.26M  2.19MB/s    in 1.5s    \n",
            "\n",
            "2024-11-18 03:29:37 (2.19 MB/s) - ‘TC_provided.tar.gz’ saved [3421989/3421989]\n",
            "\n",
            "--2024-11-18 03:29:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-11-18 03:29:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-11-18 03:29:38--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 41s  \n",
            "\n",
            "2024-11-18 03:32:18 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Download TC_provided.tar.gz\n",
        "!wget http://faculty.cooper.edu/sable2/courses/fall2024/ece467/TC_provided.tar.gz -O TC_provided.tar.gz\n",
        "\n",
        "# Extract TC_provided.tar.gz\n",
        "!tar -xzf TC_provided.tar.gz\n",
        "\n",
        "# Set the path to the extracted 'TC_provided' directory\n",
        "tc_provided_path = os.path.abspath('TC_provided')\n",
        "\n",
        "# Download GloVe embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip -O glove.6B.zip\n",
        "\n",
        "# Unzip GloVe embeddings\n",
        "!unzip -q glove.6B.zip\n",
        "\n",
        "# Set the path to 'glove.6B.100d.txt'\n",
        "glove_100d_path = os.path.abspath('glove.6B.100d.txt')\n",
        "\n",
        "# Update the 'embedding_path' in the configs\n",
        "for config in configs:\n",
        "    if config['embedding_type'] == 'glove':\n",
        "        config['embedding_path'] = glove_100d_path\n",
        "\n",
        "# Now 'tc_provided_path' and 'glove_100d_path' can be used in your code\n",
        "# For example, set 'tc' to 'tc_provided_path' in your main function\n",
        "tc = tc_provided_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTJvBIA_GDqd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import nltk\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from torch.optim import Adam\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "np.random.seed(13)\n",
        "torch.manual_seed(13)\n",
        "\n",
        "\n",
        "class LoadDataset:\n",
        "    def __init__(self, base_path):\n",
        "        self.base_path = base_path\n",
        "        self.train_label_file = \"corpus1_train.labels\"\n",
        "        self.test_label_file = \"corpus1_test.labels\"\n",
        "        self.data = self.load_data()\n",
        "\n",
        "    def load_dataset(self, label_file_name, dataset_type):\n",
        "        label_file = os.path.join(self.base_path, label_file_name)\n",
        "        print(f\"Loading {dataset_type} dataset from: {label_file}\")\n",
        "        data = []\n",
        "        with open(label_file, \"r\") as file:\n",
        "            lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            file_path = os.path.join(self.base_path, parts[0])\n",
        "            label = parts[1]\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                    text = file.read()\n",
        "                data.append({\"text\": text, \"label\": label})\n",
        "            except FileNotFoundError:\n",
        "                print(\n",
        "                    f\"Warning: The file {file_path} does not exist and will be skipped.\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_path}: {e}\")\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def load_data(self):\n",
        "        train_data = self.load_dataset(self.train_label_file, \"Training\")\n",
        "        test_data = self.load_dataset(self.test_label_file, \"Testing\")\n",
        "        return {\"train\": train_data, \"test\": test_data}\n",
        "\n",
        "\n",
        "class Processor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        datasets,\n",
        "        embedding_type=\"word2vec\",  # 'word2vec', 'glove', 'fasttext', 'random'\n",
        "        embedding_dim=100,  # Embedding dimension\n",
        "        embedding_path=None,  # Path to embeddings (GloVe or FastText)\n",
        "        batch_size=32,\n",
        "        shuffle_train=True,\n",
        "        verbose=False,\n",
        "        max_seq_length=100,\n",
        "    ):\n",
        "        self.train_data = datasets.data[\"train\"]\n",
        "        self.test_data = datasets.data[\"test\"]\n",
        "        self.embed_dim = embedding_dim\n",
        "        self.embedding_type = embedding_type\n",
        "        self.embedding_path = embedding_path\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.model = None\n",
        "        self.vocabulary = {}\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle_train = shuffle_train\n",
        "        self.verbose = verbose\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Process data\n",
        "        self.train_sentences, self.test_sentences = self.preprocess_datasets()\n",
        "\n",
        "        if self.embedding_type == \"word2vec\":\n",
        "            self.train_word2vec_model()\n",
        "            self.get_vocabulary_word2vec()\n",
        "            self.create_embeddings_matrix_word2vec()\n",
        "        elif self.embedding_type in [\"glove\", \"fasttext\"]:\n",
        "            self.build_vocabulary_from_data()\n",
        "            self.load_embeddings()\n",
        "            self.create_embeddings_matrix()\n",
        "        elif self.embedding_type == \"random\":\n",
        "            self.build_vocabulary_from_data()\n",
        "            self.embeddings_matrix = None  # No pre-trained embeddings\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Invalid embedding_type. Must be 'word2vec', 'glove', 'fasttext', or 'random'.\"\n",
        "            )\n",
        "\n",
        "        self.encode_labels()\n",
        "        self.create_sequences()\n",
        "        self.X_train_tensors = self.to_tensors(self.X_train_padded, dtype=torch.long)\n",
        "        self.X_test_tensors = self.to_tensors(self.X_test_padded, dtype=torch.long)\n",
        "        self.y_train_tensors = self.to_tensors(self.y_train_encoded, dtype=torch.long)\n",
        "        self.y_test_tensors = self.to_tensors(self.y_test_encoded, dtype=torch.long)\n",
        "        self.training_data_loader, self.testing_data_loader = self.get_loaders()\n",
        "\n",
        "        if self.verbose:\n",
        "            self.display_info()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "        tokens = word_tokenize(text)\n",
        "        cleaned_tokens = []\n",
        "        for token in tokens:\n",
        "            if token not in self.stop_words and token.strip():\n",
        "                cleaned_tokens.append(token)\n",
        "        return cleaned_tokens\n",
        "\n",
        "    def preprocess_datasets(self):\n",
        "        self.train_sentences = []\n",
        "        for text in self.train_data[\"text\"]:\n",
        "            self.train_sentences.append(self.preprocess_text(text))\n",
        "\n",
        "        self.test_sentences = []\n",
        "        for text in self.test_data[\"text\"]:\n",
        "            self.test_sentences.append(self.preprocess_text(text))\n",
        "\n",
        "        return self.train_sentences, self.test_sentences\n",
        "\n",
        "    def train_word2vec_model(self):\n",
        "        self.model = Word2Vec(\n",
        "            self.train_sentences,\n",
        "            vector_size=self.embed_dim,\n",
        "            window=5,\n",
        "            min_count=1,\n",
        "            workers=4,\n",
        "        )\n",
        "\n",
        "    def get_vocabulary_word2vec(self):\n",
        "        self.vocabulary = {}\n",
        "        for idx, word in enumerate(self.model.wv.index_to_key):\n",
        "            self.vocabulary[word] = idx + 1\n",
        "\n",
        "    def build_vocabulary_from_data(self):\n",
        "        all_tokens = set()\n",
        "        for tokens in self.train_sentences:\n",
        "            all_tokens.update(tokens)\n",
        "\n",
        "        self.vocabulary = {}\n",
        "        for idx, word in enumerate(all_tokens):\n",
        "            self.vocabulary[word] = idx + 1\n",
        "\n",
        "    def load_embeddings(self):\n",
        "        if self.embedding_path is None:\n",
        "            raise ValueError(\n",
        "                f\"embedding_path must be specified when embedding_type is '{self.embedding_type}'.\"\n",
        "            )\n",
        "        self.embeddings_index = {}\n",
        "        with open(self.embedding_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                values = line.rstrip().split(\" \")\n",
        "                word = values[0]\n",
        "                coeffs = np.asarray(values[1:], dtype=\"float32\")\n",
        "                self.embeddings_index[word] = coeffs\n",
        "\n",
        "    def create_embeddings_matrix(self):\n",
        "        vocab_size = len(self.vocabulary) + 1\n",
        "        embeddings_matrix = np.zeros((vocab_size, self.embed_dim))\n",
        "        for word, idx in self.vocabulary.items():\n",
        "            embedding_vector = self.embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embeddings_matrix[idx] = embedding_vector\n",
        "            else:\n",
        "                embeddings_matrix[idx] = np.random.normal(\n",
        "                    scale=0.6, size=(self.embed_dim,)\n",
        "                )\n",
        "        self.embeddings_matrix = torch.tensor(embeddings_matrix, dtype=torch.float32)\n",
        "\n",
        "    def create_embeddings_matrix_word2vec(self):\n",
        "        vocab_size = len(self.vocabulary) + 1\n",
        "        embeddings_matrix = np.zeros((vocab_size, self.embed_dim))\n",
        "        for word, idx in self.vocabulary.items():\n",
        "            embeddings_matrix[idx] = self.model.wv[word]\n",
        "        self.embeddings_matrix = torch.tensor(embeddings_matrix, dtype=torch.float32)\n",
        "\n",
        "    def encode_labels(self):\n",
        "        self.y_train_encoded = self.label_encoder.fit_transform(\n",
        "            self.train_data[\"label\"]\n",
        "        )\n",
        "        self.y_test_encoded = self.label_encoder.transform(self.test_data[\"label\"])\n",
        "\n",
        "    def tokens_to_indices(self, tokens):\n",
        "        indices = []\n",
        "        for word in tokens:\n",
        "            indices.append(self.vocabulary.get(word, 0))\n",
        "        return indices\n",
        "\n",
        "    def pad_sequences(self, sequences, max_len):\n",
        "        padded_sequences = []\n",
        "        for seq in sequences:\n",
        "            if len(seq) < max_len:\n",
        "                seq.extend([0] * (max_len - len(seq)))\n",
        "            else:\n",
        "                seq = seq[:max_len]\n",
        "            padded_sequences.append(seq)\n",
        "        return padded_sequences\n",
        "\n",
        "    def create_sequences(self):\n",
        "        self.X_train_indices = []\n",
        "        for tokens in self.train_sentences:\n",
        "            self.X_train_indices.append(self.tokens_to_indices(tokens))\n",
        "\n",
        "        self.X_test_indices = []\n",
        "        for tokens in self.test_sentences:\n",
        "            self.X_test_indices.append(self.tokens_to_indices(tokens))\n",
        "\n",
        "        self.X_train_padded = self.pad_sequences(\n",
        "            self.X_train_indices, self.max_seq_length\n",
        "        )\n",
        "        self.X_test_padded = self.pad_sequences(\n",
        "            self.X_test_indices, self.max_seq_length\n",
        "        )\n",
        "\n",
        "    def to_tensors(self, data, dtype):\n",
        "        return torch.tensor(data, dtype=dtype)\n",
        "\n",
        "    def get_loaders(self):\n",
        "        train_dataset = TensorDataset(self.X_train_tensors, self.y_train_tensors)\n",
        "        test_dataset = TensorDataset(self.X_test_tensors, self.y_test_tensors)\n",
        "        training_data_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=self.shuffle_train,\n",
        "        )\n",
        "        testing_data_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "        )\n",
        "        return training_data_loader, testing_data_loader\n",
        "\n",
        "    def display_info(self):\n",
        "        from tabulate import tabulate\n",
        "\n",
        "        data = [\n",
        "            [\"X_train\", self.X_train_tensors.shape],\n",
        "            [\"X_test\", self.X_test_tensors.shape],\n",
        "            [\"y_train\", self.y_train_tensors.shape],\n",
        "            [\"y_test\", self.y_test_tensors.shape],\n",
        "            [\"Vocabulary Size\", len(self.vocabulary)],\n",
        "            [\"Embeddings Matrix\", self.embeddings_matrix.shape],\n",
        "        ]\n",
        "        print(tabulate(data, headers=[\"Dataset\", \"Shape\"], tablefmt=\"grid\"))\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        no_layers,\n",
        "        hidden_dim,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        embeddings=None,\n",
        "        output_dim=1,\n",
        "        freeze_embeddings=True,\n",
        "        bidirectional=False,\n",
        "        dropout=0.5,\n",
        "    ):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.no_layers = no_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        if embeddings is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                embeddings, freeze=freeze_embeddings\n",
        "            )\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(\n",
        "                num_embeddings=vocab_size + 1,  # Adding 1 if padding_idx=0\n",
        "                embedding_dim=embedding_dim,\n",
        "                padding_idx=0,  # Assuming index 0 is for padding\n",
        "            )\n",
        "            if freeze_embeddings:\n",
        "                self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=no_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * self.num_directions, output_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x, hidden = self.lstm(x, hidden)\n",
        "        x = self.dropout(x[:, -1, :])\n",
        "        x = self.fc(x)\n",
        "        return x, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        h0 = torch.zeros(\n",
        "            self.no_layers * self.num_directions, batch_size, self.hidden_dim\n",
        "        ).to(device)\n",
        "        c0 = torch.zeros(\n",
        "            self.no_layers * self.num_directions, batch_size, self.hidden_dim\n",
        "        ).to(device)\n",
        "        return (h0, c0)\n",
        "\n",
        "\n",
        "def get_model(vocab_size, embeddings, config, device=\"cpu\", output_dim=5):\n",
        "    net = LSTM(\n",
        "        no_layers=config.get(\"no_layers\", 2),\n",
        "        hidden_dim=config.get(\"hidden_dim\", 128),\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=config.get(\"embedding_dim\", 100),\n",
        "        embeddings=embeddings,\n",
        "        output_dim=output_dim,\n",
        "        freeze_embeddings=config.get(\"freeze_embeddings\", True),\n",
        "        bidirectional=config.get(\"bidirectional\", False),\n",
        "        dropout=config.get(\"dropout\", 0.5),\n",
        "    ).to(device)\n",
        "\n",
        "    loss_fun = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        net.parameters(), lr=config.get(\"learning_rate\", 0.001)\n",
        "    )\n",
        "\n",
        "    return net, loss_fun, optimizer\n",
        "\n",
        "\n",
        "def train_model(processor, config, device=\"cpu\"):\n",
        "    # Get data loaders from the processor\n",
        "    training_data_loader = processor.training_data_loader\n",
        "    testing_data_loader = processor.testing_data_loader\n",
        "\n",
        "    # Get embeddings matrix and vocab size\n",
        "    embeddings = processor.embeddings_matrix\n",
        "    vocab_size = len(processor.vocabulary)\n",
        "\n",
        "    # Determine output dimension (number of classes)\n",
        "    output_dim = len(processor.label_encoder.classes_)\n",
        "\n",
        "    # Initialize the model, loss function, and optimizer\n",
        "    net, loss_fun, optimizer = get_model(\n",
        "        vocab_size=vocab_size,\n",
        "        embeddings=embeddings,\n",
        "        config=config,\n",
        "        device=device,\n",
        "        output_dim=output_dim,\n",
        "    )\n",
        "    net.to(device)\n",
        "\n",
        "    # Training parameters\n",
        "    clip = config.get(\"clip\", 5)\n",
        "    epochs = config.get(\"epochs\", 50)\n",
        "\n",
        "    train_acc_list, train_loss_list = [], []\n",
        "    test_acc_list, test_loss_list = [], []\n",
        "\n",
        "    # Initialize the progress bar without 'ascii=True' to use Unicode characters\n",
        "    progress_bar = tqdm(total=epochs, desc=\"Training\", leave=True, file=sys.stdout)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        batch_acc, batch_loss = [], []\n",
        "\n",
        "        for X_batch, y_batch in training_data_loader:\n",
        "            current_batch_size = X_batch.size(0)\n",
        "            h = net.init_hidden(current_batch_size, device)\n",
        "\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat, h = net(X_batch, h)\n",
        "            loss = loss_fun(y_hat, y_batch)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            predictions = torch.argmax(y_hat, dim=1)\n",
        "            accuracy = (predictions == y_batch).float().mean()\n",
        "            batch_acc.append(accuracy.item())\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "        # Compute average training accuracy and loss for the epoch\n",
        "        train_acc_epoch = np.mean(batch_acc)\n",
        "        train_loss_epoch = np.mean(batch_loss)\n",
        "        train_acc_list.append(train_acc_epoch)\n",
        "        train_loss_list.append(train_loss_epoch)\n",
        "\n",
        "        net.eval()\n",
        "        batch_acc, batch_loss = [], []\n",
        "\n",
        "        for X_batch, y_batch in testing_data_loader:\n",
        "            current_batch_size = X_batch.size(0)\n",
        "            h = net.init_hidden(current_batch_size, device)\n",
        "\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            with torch.no_grad():\n",
        "                y_hat, h = net(X_batch, h)\n",
        "                loss = loss_fun(y_hat, y_batch)\n",
        "                predictions = torch.argmax(y_hat, dim=1)\n",
        "                accuracy = (predictions == y_batch).float().mean()\n",
        "                batch_acc.append(accuracy.item())\n",
        "                batch_loss.append(loss.item())\n",
        "\n",
        "        # Compute average testing accuracy and loss for the epoch\n",
        "        test_acc_epoch = np.mean(batch_acc)\n",
        "        test_loss_epoch = np.mean(batch_loss)\n",
        "        test_acc_list.append(test_acc_epoch)\n",
        "        test_loss_list.append(test_loss_epoch)\n",
        "\n",
        "        # Format the metrics in a readable way\n",
        "        metrics_str = (\n",
        "            f\"Epoch {epoch + 1}/{epochs} | \"\n",
        "            f\"Train Acc: {train_acc_epoch:.4f} | \"\n",
        "            f\"Train Loss: {train_loss_epoch:.4f} | \"\n",
        "            f\"Test Acc: {test_acc_epoch:.4f} | \"\n",
        "            f\"Test Loss: {test_loss_epoch:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update the progress bar's postfix to display metrics\n",
        "        progress_bar.set_postfix_str(metrics_str)\n",
        "\n",
        "        # Manually update the progress bar\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Close the progress bar after completion\n",
        "    progress_bar.close()\n",
        "\n",
        "    return train_acc_list, test_acc_list, train_loss_list, test_loss_list, net\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    dataset = LoadDataset(tc)\n",
        "\n",
        "    # Run experiments\n",
        "    for config in configs:\n",
        "        print(f\"Running Experiment: {config['experiment_name']}\")\n",
        "\n",
        "        processor = Processor(\n",
        "            datasets=dataset,\n",
        "            embedding_type=config.get(\"embedding_type\", \"word2vec\"),\n",
        "            embedding_dim=config.get(\"embedding_dim\", 100),\n",
        "            embedding_path=config.get(\"embedding_path\", None),\n",
        "            batch_size=32,\n",
        "            shuffle_train=True,\n",
        "            verbose=False,\n",
        "            max_seq_length=100,\n",
        "        )\n",
        "\n",
        "        # Get device (CPU or GPU)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Train the model\n",
        "        train_acc, test_acc, train_loss, test_loss, net = train_model(\n",
        "            processor, config, device=device\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}