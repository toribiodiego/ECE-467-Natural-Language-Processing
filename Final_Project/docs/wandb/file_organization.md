# W&B File Organization

This document explains how files are organized in the Weights & Biases Files tab after training.

## Directory Structure

When you complete a training run, the W&B Files tab will contain:

```
files/
├── checkpoint/
│   └── {model-name-timestamp}/         # e.g., distilbert-12-11-2025-221314/
│       ├── pytorch_model.bin           # Model weights (~265 MB for DistilBERT)
│       ├── config.json                 # Model configuration
│       ├── tokenizer.json              # Tokenizer
│       ├── vocab.txt                   # Vocabulary
│       ├── special_tokens_map.json     # Special tokens
│       ├── tokenizer_config.json       # Tokenizer config
│       └── metrics.json                # Training metrics summary
│
├── predictions/                         # Predictions with probabilities
│   ├── test_predictions_{model}_{timestamp}.csv
│   └── val_epoch{N}_predictions_{model}_{timestamp}.csv
│
├── stats/                               # Per-class performance metrics
│   └── per_class_metrics_{model}_{timestamp}.csv
│
├── artifact/                            # W&B internal artifact tracking
│   ├── {artifact_id}/                  # (auto-generated by W&B)
│   │   └── wandb_manifest.json
│   └── ...                             # Multiple artifact folders
│
└── [W&B metadata files]                 # Auto-generated by W&B
    ├── config.yaml                     # Run configuration
    ├── output.log                      # Training logs
    ├── requirements.txt                # Python dependencies
    ├── wandb-metadata.json             # System metadata
    └── wandb-summary.json              # Run summary
```

## File Categories

### 1. Your Training Outputs (Organized)

These are the files you care about for analysis and model deployment.

#### Checkpoint Directory

**Location**: `{model-name-timestamp}/`

**Contents**:
- `pytorch_model.bin` - Model weights (binary file, ~265 MB for DistilBERT)
- `config.json` - Model configuration and architecture
- `tokenizer.json` - Tokenizer with vocabulary mappings
- `vocab.txt` - Vocabulary file
- `special_tokens_map.json` - Special tokens configuration
- `tokenizer_config.json` - Tokenizer settings
- `metrics.json` - Training metrics summary

**Size**: ~265 MB for DistilBERT, ~440 MB for BERT-base

**Use cases**:
- Load model for inference
- Continue training from checkpoint
- Deploy model to production
- Share model with collaborators

**Example - Load checkpoint**:
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained('./checkpoint_dir')
tokenizer = AutoTokenizer.from_pretrained('./checkpoint_dir')
```

#### Predictions CSVs

**Location**: `predictions/`

**Files**:
- `test_predictions_{model}_{timestamp}.csv` - Final test set predictions
- `val_epoch{N}_predictions_{model}_{timestamp}.csv` - Validation predictions per epoch

**Contents**:
- `text` - Input text
- `true_labels` - Ground truth emotion labels
- `pred_labels` - Predicted emotion labels
- `pred_prob_{emotion}` - Probability for each of 28 emotions

**Size**: ~8-10 KB per file (for small test runs), can be larger for full datasets

**Use cases**:
- Error analysis (which predictions were wrong?)
- Threshold tuning (adjust prediction threshold)
- Ablation studies (compare predictions across models)
- Qualitative analysis (review specific examples)

**Example - Analyze predictions**:
```python
import pandas as pd

df = pd.read_csv('predictions/test_predictions_*.csv')

# Find misclassified examples
df['correct'] = df['true_labels'] == df['pred_labels']
errors = df[~df['correct']]

print(f'Accuracy: {df["correct"].mean():.2%}')
print(f'Errors: {len(errors)}')
```

#### Metrics CSV

**Location**: `stats/`

**File**: `per_class_metrics_{model}_{timestamp}.csv`

**Columns**:
- `emotion` - Emotion name
- `f1_score` - F1 score for this emotion
- `precision` - Precision for this emotion
- `recall` - Recall for this emotion
- `support` - Number of samples in test set
- `tp`, `fp`, `fn`, `tn` - Confusion matrix values

**Size**: ~1-2 KB

**Use cases**:
- Identify which emotions are hardest to predict
- Compare per-emotion performance across models
- Generate performance reports
- Create visualizations

**Example - Find worst performing emotions**:
```python
import pandas as pd

df = pd.read_csv('stats/per_class_metrics_*.csv')
worst = df.nsmallest(5, 'f1_score')[['emotion', 'f1_score', 'support']]
print(worst)
```

### 2. W&B Metadata Files (Auto-Generated)

These files are automatically saved by W&B and are useful for reproducibility and debugging.

#### config.yaml

**Contents**: Complete run configuration including:
- All hyperparameters (learning rate, batch size, epochs, etc.)
- Model configuration (architecture, dropout, etc.)
- Dataset configuration (train/val/test sizes)
- System information

**Use**: Reproduce exact training configuration

#### output.log

**Contents**: Full training output logs including:
- Epoch-by-epoch progress
- Loss values per batch
- Validation results
- Error messages

**Use**: Debug training issues, review training progression

#### requirements.txt

**Contents**: All Python packages and versions used during training

**Use**: Recreate exact environment for reproducibility

#### wandb-metadata.json

**Contents**: System information including:
- OS and Python version
- PyTorch and transformers versions
- CUDA version and GPU info
- Git commit hash

**Use**: Track system dependencies and environment

#### wandb-summary.json

**Contents**: Final summary metrics including:
- Best epoch
- Final test metrics
- Total training time
- Per-class test metrics

**Use**: Quick access to final results without loading full run

### 3. W&B Artifact Tracking (Internal)

#### artifact/ directory

**Contents**: W&B's internal artifact tracking files
- `artifact/{artifact_id}/wandb_manifest.json` - Artifact metadata

**Purpose**: W&B uses these to track artifact versions and metadata

**Important**: These files are created automatically by W&B's artifact system. You can ignore them - your organized outputs (checkpoint, predictions, stats) are separate and clean.

**Why they exist**: When you upload artifacts (checkpoint, predictions, metrics), W&B creates manifest files to track:
- Which files are in the artifact
- File checksums for integrity
- Artifact versioning info
- Lineage tracking

## File Sizes

Typical file sizes for a training run:

| File | Size (DistilBERT) | Size (BERT-base) |
|------|------------------|------------------|
| pytorch_model.bin | ~265 MB | ~440 MB |
| tokenizer.json | ~700 KB | ~700 KB |
| vocab.txt | ~230 KB | ~230 KB |
| config.json | ~2 KB | ~2 KB |
| metrics.json | ~7 KB | ~7 KB |
| test_predictions.csv | ~8-50 KB | ~8-50 KB |
| val_predictions.csv | ~8-50 KB | ~8-50 KB |
| per_class_metrics.csv | ~1 KB | ~1 KB |

Total per run: ~266 MB (DistilBERT), ~441 MB (BERT-base)

## Best Practices

### For Quick Access
- Use **Files tab** to browse and download individual files
- Download CSVs directly to open in Excel or pandas
- Download checkpoint to test model locally

### For Reproducibility
- Use **Artifacts tab** for versioned downloads
- Download config.yaml and requirements.txt together with checkpoint
- Link artifacts across runs for experiment tracking

### For Remote Training
1. Train on remote GPU
2. Check W&B Files tab to verify outputs synced
3. Download checkpoint and CSVs from Files tab
4. Analyze locally without needing SSH access

### For Storage Management
- W&B stores files in the cloud (free tier: 100 GB)
- Delete old runs to free space: Settings → Delete run
- Use artifacts for important checkpoints (versioned and tracked)
- Local files can be deleted after syncing to W&B

## Troubleshooting

### Files not showing up?

Check that:
1. Training completed successfully (not interrupted)
2. W&B sync finished (check logs for "Synced X file(s)")
3. You're looking at the correct run in W&B UI
4. You're logged into the correct W&B account

### Can't download large checkpoint?

Use Python API for reliable downloads:
```python
import wandb

api = wandb.Api()
run = api.run('username/GoEmotions_Classification/run_id')

# Download checkpoint files only
for file in run.files():
    if 'distilbert' in file.name and 'pytorch_model.bin' in file.name:
        file.download(root='./checkpoint')
```

### Artifact folder cluttering view?

This is normal W&B behavior. You can:
1. Ignore the artifact/ folder - focus on checkpoint/, predictions/, stats/
2. Use the search/filter in W&B UI to hide artifact files
3. Use Python API to download only specific files (see downloading_files.md)

## Additional Information

For more details on:
- **Downloading files**: See [downloading_files.md](downloading_files.md)
- **Metrics logged**: See [metrics_guide.md](metrics_guide.md)
- **W&B basics**: See [README.md](README.md)
