

> This repository contains the code for **ECE 467: Natural Language Processing**, a 3-credit graduate-level course at The Cooper Union for the Advancement of Science and Art, offering an exploration of both traditional NLP methods and modern deep learning techniques.


## Experimentation with PyTorch
**Independent Study, Spring 2025**  
**Instructor:** Professor Carl Sable   
**Syllabus:** [view](http://faculty.cooper.edu/sable2/courses/fall2024/ece467)


### Overview

This course provides a practical introduction to natural language processing, emphasizing both traditional linguistic methods and modern deep-learning approaches. Through focused study of key textbook chapters, seminal research papers, and hands-on projects, students gain the theoretical foundations and practical skills needed to build advanced NLP systems.



### Material

The primary resource for the course is the in-progress draft of [*Speech and Language Processing, 3rd Edition*](https://web.stanford.edu/~jurafsky/slp3/) by Daniel Jurafsky and James H. Martin. This textbook lays the groundwork for understanding both statistical and deep-learning methods in NLP. In addition, the curriculum integrates various published papers and online materials that cover essential topics, including:

- Traditional NLP techniques such as text categorization, syntax, and semantics.
- Deep learning fundamentals with feedforward networks, recurrent architectures (including LSTMs), and sequence-to-sequence models.
- Modern advances with transformers, BERT, GPT variations, and reinforcement learning from human feedback.


### Repository Structure


### Final Project

The project, *Multi-Label Emotion Classification Using Transformer Architectures*, investigates how different transformer models perform on the task of detecting multiple emotions in text. Leveraging the GoEmotions dataset—which captures the nuanced expression of 27 distinct emotions—the goal is to evaluate how variations in architecture, from robust models like BERT and RoBERTa to efficient alternatives like DistilBERT and SqueezeBERT, affect classification accuracy and efficiency. By comparing these approaches, the project seeks to uncover innovative strategies for optimizing transformer-based NLP systems for fine-grained emotion detection.